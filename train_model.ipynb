{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147610e8",
   "metadata": {
    "executionInfo": {
     "elapsed": 3021,
     "status": "ok",
     "timestamp": 1671896471455,
     "user": {
      "displayName": "Kiên Nguyễn Trung",
      "userId": "16521235671314957520"
     },
     "user_tz": -420
    },
    "id": "147610e8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636482d5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 283330,
     "status": "ok",
     "timestamp": 1671896755338,
     "user": {
      "displayName": "Kiên Nguyễn Trung",
      "userId": "16521235671314957520"
     },
     "user_tz": -420
    },
    "id": "636482d5",
    "outputId": "3ef8d404-73d1-42ea-a339-7641b58112f3"
   },
   "outputs": [],
   "source": [
    "annotation_folder = \"/dataset/\"\n",
    "if not os.path.exists(os.path.abspath(\".\") + annotation_folder):\n",
    "    annotation_zip = tf.keras.utils.get_file(\n",
    "        \"val.tar.gz\",\n",
    "        cache_subdir=os.path.abspath(\".\"),\n",
    "        origin=\"http://diode-dataset.s3.amazonaws.com/val.tar.gz\",\n",
    "        extract=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963ff101",
   "metadata": {
    "executionInfo": {
     "elapsed": 842,
     "status": "ok",
     "timestamp": 1671896756175,
     "user": {
      "displayName": "Kiên Nguyễn Trung",
      "userId": "16521235671314957520"
     },
     "user_tz": -420
    },
    "id": "963ff101"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcadf65",
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1671896756178,
     "user": {
      "displayName": "Kiên Nguyễn Trung",
      "userId": "16521235671314957520"
     },
     "user_tz": -420
    },
    "id": "1dcadf65"
   },
   "outputs": [],
   "source": [
    "path = 'val/indoors'\n",
    "\n",
    "filelist = []\n",
    "\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        filelist.append(os.path.join(root, file))\n",
    "\n",
    "filelist.sort()\n",
    "data = {\n",
    "    \"image\": [x for x in filelist if x.endswith('.png')],\n",
    "    \"depth\": [x for x in filelist if x.endswith('_depth.npy')],\n",
    "    \"mask\": [x for x in filelist if x.endswith('_depth_mask.npy')],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df = df.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1158044f",
   "metadata": {
    "executionInfo": {
     "elapsed": 1094,
     "status": "ok",
     "timestamp": 1671897325545,
     "user": {
      "displayName": "Kiên Nguyễn Trung",
      "userId": "16521235671314957520"
     },
     "user_tz": -420
    },
    "id": "1158044f"
   },
   "outputs": [],
   "source": [
    "HEIGHT = 256\n",
    "WIDTH = 256\n",
    "LR = 0.0002\n",
    "EPOCHS = 150\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96897f35",
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1671896756182,
     "user": {
      "displayName": "Kiên Nguyễn Trung",
      "userId": "16521235671314957520"
     },
     "user_tz": -420
    },
    "id": "96897f35"
   },
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data, batch_size=6, dim=(768, 1024), n_channels=3, shuffle=True):\n",
    "        self.data = data\n",
    "        self.indices = self.data.index.tolist()\n",
    "        self.dim = dim\n",
    "        self.n_channels = n_channels\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.min_depth = 0.1\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.data) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if (index+1) * self.batch_size > len(self.indices):\n",
    "            self.batch_size = len(self.indices) - index * self.batch_size\n",
    "        \n",
    "        index = self.indices[index * self.batch_size: (index+1)*self.batch_size]\n",
    "        \n",
    "        batch = [self.indices[k] for k in index]\n",
    "        x, y = self.data_generation(batch)\n",
    "        return x, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.index = np.arange(len(self.indices))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.index)\n",
    "    \n",
    "    def load(self, image_path, depth_map, mask):\n",
    "        image_ = cv2.imread(image_path)\n",
    "        image_ = cv2.cvtColor(image_, cv2.COLOR_BGR2RGB)\n",
    "        image_ = cv2.resize(image_, self.dim)\n",
    "        image_ = tf.image.convert_image_dtype(image_, tf.float32)\n",
    "        \n",
    "        depth_map = np.load(depth_map).squeeze()\n",
    "        \n",
    "        mask = np.load(mask)\n",
    "        mask = mask > 0\n",
    "        \n",
    "        max_depth = min(300, np.percentile(depth_map, 99))\n",
    "        depth_map = np.clip(depth_map, self.min_depth, max_depth)\n",
    "        depth_map = np.log(depth_map, where=mask)\n",
    "        \n",
    "        depth_map = np.ma.masked_where(~mask, depth_map)\n",
    "        \n",
    "        depth_map = np.clip(depth_map, 0.1, np.log(max_depth))\n",
    "        depth_map = cv2.resize(depth_map, self.dim)\n",
    "        depth_map = np.expand_dims(depth_map, axis=2)\n",
    "        depth_map = tf.image.convert_image_dtype(depth_map, tf.float32)\n",
    "        return image_, depth_map\n",
    "    \n",
    "    def data_generation(self, batch):\n",
    "        x = []\n",
    "        y = []\n",
    "        for i, batch_id in enumerate(batch):\n",
    "            x.append(self.load(\n",
    "                self.data[\"image\"][batch_id],\n",
    "                self.data[\"depth\"][batch_id],\n",
    "                self.data[\"mask\"][batch_id],\n",
    "            )[0])\n",
    "            y.append(self.load(\n",
    "                self.data[\"image\"][batch_id],\n",
    "                self.data[\"depth\"][batch_id],\n",
    "                self.data[\"mask\"][batch_id],\n",
    "            )[1])\n",
    "        x = np.array(x)\n",
    "        y = np.array(y)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cdf8b5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1Qmp3cZIMvuBpjopcl4myqtmzDmgBuP_q"
    },
    "executionInfo": {
     "elapsed": 18345,
     "status": "ok",
     "timestamp": 1671896774514,
     "user": {
      "displayName": "Kiên Nguyễn Trung",
      "userId": "16521235671314957520"
     },
     "user_tz": -420
    },
    "id": "c9cdf8b5",
    "outputId": "5826345a-a1bc-4df9-e87b-82e014d66de8"
   },
   "outputs": [],
   "source": [
    "def visualize_depth_map(samples, test=False, model=None):\n",
    "    input_img, target = samples\n",
    "\n",
    "    cmap = plt.cm.jet\n",
    "    cmap.set_bad(color='black')\n",
    "\n",
    "    if test:\n",
    "        pred = model.predict(input_img)\n",
    "        fig, ax = plt.subplots(6, 3, figsize=(50, 50))\n",
    "        for i in range(6):\n",
    "            ax[i, 0].imshow((input_img[i].squeeze()))\n",
    "            ax[i, 1].imshow((target[i].squeeze()),cmap=cmap)\n",
    "            ax[i, 2].imshow((pred[i].squeeze()), cmap=cmap)\n",
    "    else:\n",
    "        fig, ax = plt.subplots(6, 2, figsize=(50, 50))\n",
    "        for i in range(6):\n",
    "            ax[i, 0].imshow((np.squeeze(input_img[i])))\n",
    "            ax[i, 1].imshow((np.squeeze(target[i])), cmap=cmap)\n",
    "\n",
    "visualize_samples = next(iter(DataGenerator(data=df, batch_size=6, dim=(HEIGHT, WIDTH))))\n",
    "visualize_depth_map(visualize_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f39514",
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1671896774515,
     "user": {
      "displayName": "Kiên Nguyễn Trung",
      "userId": "16521235671314957520"
     },
     "user_tz": -420
    },
    "id": "44f39514"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1036a6c1",
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1671896774516,
     "user": {
      "displayName": "Kiên Nguyễn Trung",
      "userId": "16521235671314957520"
     },
     "user_tz": -420
    },
    "id": "1036a6c1"
   },
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size=(3,3), padding='same', strides=1):\n",
    "        super().__init__()\n",
    "        self.convA = layers.Conv2D(filters, kernel_size, strides, padding)\n",
    "        self.convB = layers.Conv2D(filters, kernel_size, strides, padding)\n",
    "        self.reluA = layers.LeakyReLU(alpha=.2)\n",
    "        self.reluB = layers.LeakyReLU(alpha=.2)\n",
    "        self.bn2a = tf.keras.layers.BatchNormalization()\n",
    "        self.bn2b = tf.keras.layers.BatchNormalization()\n",
    "        self.pool = layers.MaxPool2D((2, 2), (2, 2))\n",
    "        \n",
    "    def call(self, input_tensor):\n",
    "        d = self.convA(input_tensor)\n",
    "        x = self.bn2a(d)\n",
    "        x = self.reluA(x)\n",
    "        \n",
    "        x = self.convB(x)\n",
    "        x = self.bn2b(x)\n",
    "        x = self.reluB(x)\n",
    "        \n",
    "        x += d\n",
    "        p = self.pool(x)\n",
    "        return x, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e7fa99",
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1671896774517,
     "user": {
      "displayName": "Kiên Nguyễn Trung",
      "userId": "16521235671314957520"
     },
     "user_tz": -420
    },
    "id": "74e7fa99"
   },
   "outputs": [],
   "source": [
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size=(3,3), padding='same', strides=1):\n",
    "        super().__init__()\n",
    "        self.us = layers.UpSampling2D((2,2))\n",
    "        self.convA = layers.Conv2D(filters, kernel_size, strides, padding)\n",
    "        self.convB = layers.Conv2D(filters, kernel_size, strides, padding)\n",
    "        self.reluA = layers.LeakyReLU(alpha=.2)\n",
    "        self.reluB = layers.LeakyReLU(alpha=.2)\n",
    "        self.bn2a = tf.keras.layers.BatchNormalization()\n",
    "        self.bn2b = tf.keras.layers.BatchNormalization()\n",
    "        self.conc = layers.Concatenate()\n",
    "        \n",
    "    def call(self, x, skip):\n",
    "        x = self.us(x)\n",
    "        concat = self.conc([x, skip])\n",
    "        x = self.convA(concat)\n",
    "        x = self.bn2a(x)\n",
    "        x = self.reluA(x)\n",
    "        \n",
    "        x = self.convB(x)\n",
    "        x = self.bn2b(x)\n",
    "        x = self.reluB(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246dd583",
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1671896774518,
     "user": {
      "displayName": "Kiên Nguyễn Trung",
      "userId": "16521235671314957520"
     },
     "user_tz": -420
    },
    "id": "246dd583"
   },
   "outputs": [],
   "source": [
    "class BottleNeck(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size=(3,3), padding='same', strides=1):\n",
    "        super().__init__()\n",
    "        self.convA = layers.Conv2D(filters, kernel_size, strides, padding)\n",
    "        self.convB = layers.Conv2D(filters, kernel_size, strides, padding)\n",
    "        self.reluA = layers.LeakyReLU(alpha=.2)\n",
    "        self.reluB = layers.LeakyReLU(alpha=.2)\n",
    "        self.drop = layers.Dropout(rate=.4)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.convA(x)\n",
    "        x = self.reluA(x)\n",
    "        x = self.convB(x)\n",
    "        x = self.reluB(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc9c27f",
   "metadata": {
    "executionInfo": {
     "elapsed": 487,
     "status": "ok",
     "timestamp": 1671897381200,
     "user": {
      "displayName": "Kiên Nguyễn Trung",
      "userId": "16521235671314957520"
     },
     "user_tz": -420
    },
    "id": "0bc9c27f"
   },
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ssim_loss_weight = .9\n",
    "        self.l1_loss_weight = .07\n",
    "        self.edge_loss_weight = .9\n",
    "        self.loss_metric = tf.keras.metrics.Mean(name='loss')\n",
    "        self.f = [16, 32, 64, 128, 256]\n",
    "        self.encoder_block = [\n",
    "            Encoder(self.f[0]),\n",
    "            Encoder(self.f[1]),\n",
    "            Encoder(self.f[2]),\n",
    "            Encoder(self.f[3]),\n",
    "        ]\n",
    "        \n",
    "        self.bottle_neck = BottleNeck(self.f[4])\n",
    "        \n",
    "        self.decoder_block = [\n",
    "            Decoder(self.f[3]),\n",
    "            Decoder(self.f[2]),\n",
    "            Decoder(self.f[1]),\n",
    "            Decoder(self.f[0]),\n",
    "        ]\n",
    "        self.conv_layer = layers.Conv2D(1, (1,1), padding='same', activation='tanh')\n",
    "    \n",
    "    def calculate_loss(self, target, pred):\n",
    "        dy_true, dx_true = tf.image.image_gradients(target)\n",
    "        dy_pred, dx_pred = tf.image.image_gradients(pred)\n",
    "        weights_x = tf.exp(tf.reduce_mean(tf.abs(dx_true)))\n",
    "        weights_y = tf.exp(tf.reduce_mean(tf.abs(dy_true)))\n",
    "        \n",
    "        smoothness_x = dx_pred * weights_x\n",
    "        smoothness_y = dy_pred * weights_y\n",
    "        \n",
    "        depth_smoothness_loss = tf.reduce_mean(abs(smoothness_x)) + tf.reduce_mean(abs(smoothness_y))\n",
    "        ssim_loss = tf.reduce_mean(1 - tf.image.ssim(target, pred, max_val=WIDTH, filter_size=7, k1=.01**2, k2=.03**2))\n",
    "        l1_loss = tf.reduce_mean(tf.abs(target - pred))\n",
    "        \n",
    "        loss = (\n",
    "            (self.ssim_loss_weight * ssim_loss) +\n",
    "            (self.l1_loss_weight * l1_loss) +\n",
    "            (self.edge_loss_weight * depth_smoothness_loss)\n",
    "        )\n",
    "        return loss\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_metric]\n",
    "    \n",
    "    def train_step(self, batch_data):\n",
    "        input_, target = batch_data\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self(input_, training=True)\n",
    "            loss = self.calculate_loss(target, pred)\n",
    "            \n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        self.loss_metric.update_state(loss)\n",
    "        return {\n",
    "            'loss': self.loss_metric.result(),\n",
    "        }\n",
    "    \n",
    "    def test_step(self, batch_data):\n",
    "        input_, target = batch_data\n",
    "        pred = self(input_, training=False)\n",
    "        loss = self.calculate_loss(target, pred)\n",
    "        self.loss_metric.update_state(loss)\n",
    "        \n",
    "        return {'loss': self.loss_metric.result()}\n",
    "    \n",
    "    def call(self, x):\n",
    "        c1, p1 = self.encoder_block[0](x)\n",
    "        c2, p2 = self.encoder_block[1](p1)\n",
    "        c3, p3 = self.encoder_block[2](p2)\n",
    "        c4, p4 = self.encoder_block[3](p3)\n",
    "\n",
    "        bn = self.bottle_neck(p4)\n",
    "        \n",
    "        u1 = self.decoder_block[0](bn, c4)\n",
    "        u2 = self.decoder_block[1](u1, c3)\n",
    "        u3 = self.decoder_block[2](u2, c2)\n",
    "        u4 = self.decoder_block[3](u3, c1)\n",
    "        \n",
    "        return self.conv_layer(u4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0e7823",
   "metadata": {
    "id": "af0e7823",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(LR, amsgrad=False, clipnorm=1)\n",
    "\n",
    "model = Model()\n",
    "\n",
    "cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction=\"none\")\n",
    "\n",
    "model.compile(optimizer, loss=cross_entropy)\n",
    "train_loader = DataGenerator(\n",
    "    data = df[:260].reset_index(drop='true'), batch_size=BATCH_SIZE, dim=(HEIGHT, WIDTH))\n",
    "\n",
    "validation_loader = DataGenerator(\n",
    "    data=df[260:].reset_index(drop='true'), batch_size=BATCH_SIZE, dim=(HEIGHT, WIDTH))\n",
    "\n",
    "model.fit(train_loader, epochs=EPOCHS, validation_data=validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cfd871",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.history['loss'])\n",
    "plt.plot(model.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf966b6",
   "metadata": {
    "id": "0cf966b6"
   },
   "outputs": [],
   "source": [
    "test_loader = next(iter(\n",
    "    DataGenerator(data=df[265:].reset_index(drop='true'), batch_size=6, dim=(HEIGHT, WIDTH))\n",
    "))\n",
    "visualize_depth_map(test_loader, test=True, model=model)\n",
    "\n",
    "test_loader = next(iter(\n",
    "    DataGenerator(data=df[300:].reset_index(drop='true'), batch_size=6, dim=(HEIGHT, WIDTH))\n",
    "))\n",
    "visualize_depth_map(test_loader, test=True, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0364692",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6288,
     "status": "ok",
     "timestamp": 1671898142604,
     "user": {
      "displayName": "Kiên Nguyễn Trung",
      "userId": "16521235671314957520"
     },
     "user_tz": -420
    },
    "id": "d0364692",
    "outputId": "346a47c8-d100-49ae-b1a9-278bdd109a1a"
   },
   "outputs": [],
   "source": [
    "model.save('depth_estimate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa98df5e",
   "metadata": {
    "id": "aa98df5e"
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "new_model = tf.keras.models.load_model('depth_estimate')\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78da7e1",
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1671897183347,
     "user": {
      "displayName": "Kiên Nguyễn Trung",
      "userId": "16521235671314957520"
     },
     "user_tz": -420
    },
    "id": "b78da7e1"
   },
   "outputs": [],
   "source": [
    "#new_model.predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8be01ad",
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1671897183347,
     "user": {
      "displayName": "Kiên Nguyễn Trung",
      "userId": "16521235671314957520"
     },
     "user_tz": -420
    },
    "id": "b8be01ad"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
